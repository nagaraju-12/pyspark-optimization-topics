# ðŸš€ PySpark Optimization Project â€“ Banking Transactions

This project showcases practical **PySpark optimization techniques** using a synthetic **banking transactions dataset** (~5,000 records).
The goal is to build efficient, scalable data pipelines using Spark best practices.
---------------------------------------------------------------------------------------------------------

## âœ… Optimization Topics Covered

-  **Broadcast Join** â€“ Join small lookup tables without shuffle  
-  **Shuffle Partition Tuning** â€“ Control partitioning for groupBy/joins  
-  **Caching / Persisting** â€“ Improve performance on reused data  
-  **Repartition vs Coalesce** â€“ Optimize parallelism and output size  
-  **Delta Lake + Time Travel** â€“ Store and query historical data versions  
-  **Adaptive Query Execution (AQE)** â€“ Smart query plan optimization  
-  **Partitioning** â€“ Save Parquet by column folders for faster filtering  
-  **Bucketing** â€“ Reduce shuffle during joins using hash-based buckets  
--------------------------------------------------------------------------------------------------------
## Tools Used

- PySpark
- Databricks Notebook
- Delta Lake
- Parquet

